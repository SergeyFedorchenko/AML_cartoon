# ğŸ–¼ï¸ Real-to-Cartoon Image Translation with CycleGAN

Transform real-world photos into beautiful cartoon-style images using deep learning.  
This project implements an unpaired image-to-image translation pipeline using **CycleGAN**, trained to map real photos to the style of a chosen cartoon domain (e.g., Studio Ghibli).

## âœ¨ Project Overview

- **Goal**: Learn a style mapping from real photos to a cartoon domain using unpaired datasets.
- **Model**: CycleGAN (Zhu et al., 2017)
- **Domains**:
  - **Domain A**: Real-world photos (COCO, Unsplash)
  - **Domain B**: Cartoon-style images (e.g., frames from Studio Ghibli)

<img src="results/example_output.png" alt="example stylization" width="600"/>

---

## ğŸ§  Methodology

This project uses **Cycle-Consistent Adversarial Networks (CycleGAN)** to translate images between unpaired domains. The model includes:

- Generator G: `Real â†’ Cartoon`
- Generator F: `Cartoon â†’ Real`
- Discriminators: \( D_A \), \( D_B \)
- **Losses**:
  - Adversarial loss
  - Cycle-consistency loss
  - Identity loss

CycleGAN learns both directions and enforces structure retention through cycle-consistency.

---

## ğŸ—‚ï¸ Dataset

### ğŸ“· Real Photos (trainA)
- Source: [COCO](https://cocodataset.org), [Unsplash](https://unsplash.com)
- ~1000 natural photos
- Resized to 256Ã—256

### ğŸ¨ Cartoon Frames (trainB)
- Source: Screenshots from Studio Ghibli films
- ~1200 frames extracted with `ffmpeg`
- Preprocessing: Resize, crop, normalize


## ğŸ“Š 5. Results

### âœ… Qualitative Examples

The model successfully translates real photos into stylized cartoon images, preserving structural details while applying soft shading, simplified edges, and the aesthetic of the target cartoon domain (e.g., Studio Ghibli).

| Real Photo | Cartoonized Output |
|------------|--------------------|
| <img src="results/real_1.jpg" width="250"/> | <img src="results/stylized_1.jpg" width="250"/> |
| <img src="results/real_2.jpg" width="250"/> | <img src="results/stylized_2.jpg" width="250"/> |

> ğŸ” *Images are sampled from the test set using the best model checkpoint.*

---

### ğŸ“ˆ Quantitative Evaluation

We assess model performance with the **FrÃ©chet Inception Distance (FID)**, a commonly used metric to evaluate the quality and diversity of generated images relative to a target domain.

| Comparison                             | FID Score â†“ |
|----------------------------------------|-------------|
| Real photos vs Cartoon dataset         | 72.3        |
| Stylized outputs vs Cartoon dataset    | **39.6**    |

> Lower FID indicates closer resemblance to the target style.

---

### ğŸ§ª Optional: User Study

To gather subjective feedback, a small user study was conducted with 10 participants who rated the stylized outputs based on:
- **Stylistic Accuracy**
- **Image Realism**
- **Semantic Preservation**

**Average Ratings (out of 5):**
- Style Accuracy: 4.2
- Realism: 3.8
- Content Preservation: 4.4

---

### ğŸ“‰ Training Curves

Training stability is tracked using adversarial, cycle-consistency, and identity losses:

<p align="center">
  <img src="results/loss_plot.png" alt="Training Loss Curve" width="500"/>
</p>

---

### ğŸ¯ Observations

- The model performs well on scenes with clear structure (e.g. buildings, forests).
- Some artifacts appear in highly textured or abstract areas.
- Faces are preserved but may lack cartoon-like exaggeration (can be improved with UGATIT).

---

## âš ï¸ Challenges

Training an unpaired image-to-image model like CycleGAN introduces several technical and data-related difficulties:

- **GAN Instability**: Adversarial training is inherently unstable and sensitive to learning rates, weight initialization, and model capacity.
- **Data Domain Gap**: Cartoon frames may have flat colors and simplified shapes, while real-world photos include complex textures and lighting.
- **Color Bleed and Artifacts**: In early epochs, outputs often exhibit unnatural colors, checkerboard patterns, or loss of facial detail.
- **Overfitting to Backgrounds**: If the cartoon dataset lacks diversity, the generator may memorize background patterns rather than learning style features.

> These issues were mitigated through careful data preprocessing, early stopping, and cycle-consistency regularization.

---

## ğŸ“ References

- Zhu, J., Park, T., Isola, P., & Efros, A. (2017). *Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks*. ICCV. [[Paper]](https://arxiv.org/abs/1703.10593)
- Gatys, L. A., Ecker, A. S., & Bethge, M. (2016). *Image Style Transfer Using Convolutional Neural Networks*. CVPR. [[Paper]](https://arxiv.org/abs/1508.06576)
- Kim, H., & Yu, J. (2020). *U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization*. ICLR. [[Paper]](https://arxiv.org/abs/1907.10830)
- Chen, Y. J. et al. (2018). *CartoonGAN: Generative Adversarial Networks for Photo Cartoonization*. CVPR. [[Paper]](https://arxiv.org/abs/1804.02857)

---

## ğŸ“¬ Contact

If you have questions or want to collaborate, feel free to reach out:

- ğŸ“§ Email: `[your_email@example.com]`
- ğŸ™ GitHub Issues: [Open an issue](https://github.com/your_username/your_repo/issues)

---

